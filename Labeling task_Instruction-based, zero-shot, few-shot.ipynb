{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9e0d30",
   "metadata": {},
   "source": [
    "## Importing OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff19da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "api_key =\"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\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a4269",
   "metadata": {},
   "source": [
    "## Labeling task: instruction-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bb579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "data = pd.read_csv(\"Dataset/SignleFR_questions3.csv\")\n",
    "\n",
    "data_p_df = pd.DataFrame(data)\n",
    "data_p_df = data_p_df.fillna('\"\"')\n",
    "\n",
    "### first section of the prompot\n",
    "       \n",
    "sec1 = f\"\"\"If the functional roles as lists of \"Measure (M)\",\\\n",
    "\"Condition (C)\", \"Support (S)\", \"Spatial extent (SE)\", and\\\n",
    "\"Temporal extent (TE)\". Also, Spatial relation (SR) can present\\\n",
    "in the functional roles of C, S, SE, they are denoted as SRc, SRs,\\\n",
    "SRse, respectively. Also, Temporal Relation (TR) can present in the\\\n",
    "functional roles of C, S, TE, they are denoted as TRc, TRs, TRte,\\\n",
    "respectively. Also, Time (T) or Place (P) can only be present in the\\\n",
    "functional role of M, they are denoted as Tm and Pm, respectively.\\\n",
    "Also, SR in here is based on 'Cardinal Direction Calculus (CDC)' or\\\n",
    "'Region Connection Calculus (RCC)'. Also, TR is based on 'Allen Interval Algebra (AIA)'.\"\"\"\n",
    "    \n",
    "sec5 = ''\n",
    "sec4 = ''\n",
    "sec6 = ''\n",
    "sec7 = ''\n",
    "### second section of the prompt: Choosing one sample query has both SE and TE \n",
    "pattern = data_p_df.iloc[31]   \n",
    "pattern = pattern.fillna('\"\"')\n",
    "\n",
    "main_query = pattern['Reformulated Question']\n",
    "\n",
    "\n",
    "sec2 = \"In the question, '{0}', the functional roles are identified as below:\\\n",
    "    \\n- M: {1}\\n- C: {2}\\n- S: {3}\\n- SE: {4}\\n- TE {5}\\\n",
    "    \\nAlso, the spatial and temporal relations in these roles are identified in the given question as below:\\\n",
    "    \\n- Tm: {6}\\n- Pm: {7}\\n- SRc: {8}\\n- TRc: {9}\\n- SRs: {10}\\n- TRs: {11}\\n- SRse: {12}\\n- TRte: {13}\".format(\n",
    "        main_query, pattern['M'],\\\n",
    "        pattern['C'], pattern['S'],\\\n",
    "        pattern['SE'], pattern['TE'],\\\n",
    "        pattern['Tm'], pattern['Pm'],\\\n",
    "        pattern['SRc'], pattern['TRc'],\\\n",
    "        pattern['SRs'], pattern['TRs'],\\\n",
    "        pattern['SRse'], pattern['TRte'])\n",
    "\n",
    "  \n",
    "    \n",
    "### Third section of the prompt: instruct based on training set\n",
    "prompt = []\n",
    "training = [data.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'train']\n",
    "training_df = pd.DataFrame(training)\n",
    "\n",
    "for index, row in training_df.iterrows():\n",
    "    \n",
    "    Notnan_columns = [i for i in row[~row.isna()].index.tolist() if i !='Reformulated Question' \\\n",
    "                          and i !='Pattern' and i != 'Class'and i != 'train-test']\n",
    "    sec3 = ''\n",
    "\n",
    "    for i in Notnan_columns:\n",
    "        sec3 += \"{0}: {1}\\n\".format(i, row[i])\n",
    "\n",
    "    sec4 += \"In the question, '{0}', the functional roles are as below:\\n{1}\\n\".format(row['Reformulated Question'],sec3)\n",
    "\n",
    "sec5 = \"{0}\\n\\n{1}\\n\\n{2}\".format(sec1, sec2, sec4)\n",
    "    \n",
    "### fourth section of the prompt: ask to label based on test set\n",
    "\n",
    "test = [data.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'test']\n",
    "test_df = pd.DataFrame(test)\n",
    "response_test = []\n",
    "for index, row in test_df.iterrows():\n",
    " \n",
    "    sec6 = \"In the question, '{0}', the functional roles are as below:\\n\".format(row['Reformulated Question'])\n",
    "    sec7 = sec5 + sec6 \n",
    "    response = get_completion(sec7)\n",
    "    response_test.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cedf72fe-a626-4938-a6d7-9a2cb45cb6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the functional roles as lists of \"Measure (M)\",\"Condition (C)\", \"Support (S)\", \"Spatial extent (SE)\", and\"Temporal extent (TE)\". Also, Spatial relation (SR) can presentin the functional roles of C, S, SE, they are denoted as SRc, SRs,SRse, respectively. Also, Temporal Relation (TR) can present in thefunctional roles of C, S, TE, they are denoted as TRc, TRs, TRte,respectively. Also, Time (T) or Place (P) can only be present in thefunctional role of M, they are denoted as Tm and Pm, respectively.Also, SR in here is based on 'Cardinal Direction Calculus (CDC)' or'Region Connection Calculus (RCC)'. Also, TR is based on 'Allen Interval Algebra (AIA)'.\n",
      "\n",
      "In the question, 'What are the number  of visitors go to the park  for each area after 2019?', the functional roles are identified as below:    \n",
      "- M: the number\n",
      "- C: visitors go to the park\n",
      "- S: each area\n",
      "- SE: \"\"\n",
      "- TE after 2019    \n",
      "Also, the spatial and temporal relations in these roles are identified in the given question as below:    \n",
      "- Tm: \"\"\n",
      "- Pm: \"\"\n",
      "- SRc: \"\"\n",
      "- TRc: \"\"\n",
      "- SRs: \"\"\n",
      "- TRs: \"\"\n",
      "- SRse: \"\"\n",
      "- TRte: after\n",
      "\n",
      "In the question, 'What is the Australian independence day?', the functional roles are as below:\n",
      "Tm: the Australian independence day\n",
      "\n",
      "In the question, 'What is the date of deadliest huricane for each US state?', the functional roles are as below:\n",
      "C: deadliest huricane\n",
      "S: US state\n",
      "Tm: the date\n",
      "\n",
      "In the question, 'What is the deadliest hurricane location?', the functional roles are as below:\n",
      "Pm: deadliest hurricane location\n",
      "\n",
      "In the question, 'What is the location and date  of the deadliest huricane  for each state ?', the functional roles are as below:\n",
      "C: the deadliest huricane\n",
      "S: state\n",
      "Tm: date\n",
      "Pm: the location\n",
      "\n",
      "In the question, 'What areas  are within Amsterdam  with the highest crime rate between peak hours for each specific category of crime?', the functional roles are as below:\n",
      "M: areas\n",
      "C: within Amsterdam, the highest crime rate, between peak hours\n",
      "S: specific category of crime\n",
      "SRc: within\n",
      "TRc: between\n",
      "\n",
      "In the question, 'What are the number  of visitors go to the park  for east of each european countries ?', the functional roles are as below:\n",
      "M: the number\n",
      "C: visitors go to the park\n",
      "S: east of each european countries\n",
      "SRs: east of\n",
      "\n",
      "In the question, 'What are the number  of visitors go to the park  for east of each european countries  every day?', the functional roles are as below:\n",
      "M: the number\n",
      "C: visitors go to the park\n",
      "S: east of each european countries, every day\n",
      "SRs: east of\n",
      "TRs: every\n",
      "\n",
      "In the question, 'What buildings  are affected by a hurricane  for each state in US?', the functional roles are as below:\n",
      "M: buildings\n",
      "C: affected by a hurricane \n",
      "S: each state\n",
      "SE: in US\n",
      "SRse: in\n",
      "\n",
      "In the question, 'What are the number  of visitors go to the park  for each area after 2019?', the functional roles are as below:\n",
      "M: the number\n",
      "C: visitors go to the park\n",
      "S: each area\n",
      "TE: after 2019\n",
      "TRte: after\n",
      "\n",
      "In the question, 'What is the death rate of infants for each suburb in Melbourne in 2020?', the functional roles are as below:\n",
      "M: the death rate\n",
      "C: infants\n",
      "S: each suburb\n",
      "SE: in Melbourne\n",
      "TE: in 2020\n",
      "SRse: in\n",
      "TRte: in\n",
      "\n",
      "In the question, 'What lanmarks  are north of Amsterdam?', the functional roles are as below:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sec7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "afafca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'Tm', 'Pm']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"Dataset/SignleFR_questions3.csv\")\n",
    "\n",
    "# data_p_df = pd.DataFrame(data)\n",
    "\n",
    "# [data_p_df.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'test'\\\n",
    "#             and data_p_df.loc[i][m] != '\"\"']\n",
    "\n",
    "# [i for i in data_p_df.iloc[10][~data_p_df.iloc[10].isna()].index.tolist() if i !='Reformulated Question' \\\n",
    "#                           and i !='Pattern' and i != 'Class'and i != 'train-test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830430eb",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ea0ad7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 8, 0, 0),\n",
       " (8, 14, 0, 0),\n",
       " (5, 8, 0, 0),\n",
       " (2, 2, 0, 0),\n",
       " (6, 1, 0, 0),\n",
       " (1, 2, 0, 0),\n",
       " (0, 2, 0, 0),\n",
       " (2, 2, 0, 0),\n",
       " (2, 2, 0, 0),\n",
       " (2, 0, 0, 0),\n",
       " (1, 0, 0, 0),\n",
       " (2, 0, 0, 0),\n",
       " (3, 1, 0, 0)]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags = [i for i in list(data_p_df.columns.values) if i !='Reformulated Question' \\\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# data = ['Tm: the date', 'C: the battle of hundred slain Fetterman Massacre']\n",
    "a = []\n",
    "\n",
    "test = [data_p_df.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'test']\n",
    "test_df = pd.DataFrame(test)\n",
    "\n",
    "metrics = []\n",
    "for m in tags:\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    for index, i in enumerate(response_test):\n",
    "\n",
    "        data = i.split(\"\\n\")\n",
    "        # Split each string into key-value pairs\n",
    "        \n",
    "\n",
    "        key_value_pairs = [item.split(':') for item in data]\n",
    "\n",
    "        # Create a dictionary from the key-value pairs\n",
    "        dictionary_data = {key.strip(): value.strip() for key, value in key_value_pairs}\n",
    "        \n",
    "        if m in dictionary_data.keys():\n",
    "            \n",
    "            test_tag = test_df.iloc[index]\n",
    "            \n",
    "            if dictionary_data[m] == test_tag[m] and dictionary_data[m] != '\"\"':\n",
    "                tp = tp+1\n",
    "            elif dictionary_data[m] != test_tag[m] and  test_tag[m] != '\"\"':\n",
    "                fp = fp+1\n",
    "            elif dictionary_data[m] != test_tag[m] and  test_tag[m] == '\"\"':\n",
    "                fn = fn+1\n",
    "            else:\n",
    "                tn = tn+1\n",
    "                    \n",
    "    metrics.append((tp, fp, fn,tn))\n",
    "                    \n",
    "                \n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9229ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## evaluation metrics when the model is prompted with train set and asked for the test\n",
    "\n",
    "# def correct_entities(test, tag):\n",
    "    \n",
    "#     test = [data.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'test'\\\n",
    "#             and data_p_df.loc[i][tag] != '\"\"']\n",
    "    \n",
    "#     test_df = pd.DataFrame(test)\n",
    "    \n",
    "#     correct_tags = []\n",
    "#     for index, row in test_df.iterrows():\n",
    "#         Notnan_columns = [i for i in row[~row.isna()].index.tolist() if i !='Reformulated Question' \\\n",
    "#                           and i !='Pattern' and i != 'Class'and i != 'train-test']\n",
    "        \n",
    "#         [response_test.iloc[i] for i in range(len(response_test)) if response_test.iloc[i]['train-test'] == 'test'\\\n",
    "#             and response_test.iloc[i]['M'] != '\"\"' ]\n",
    "#         sec = ''\n",
    "\n",
    "#         for i in Notnan_columns:\n",
    "#             sec += \"- {0}: {1}\\n\".format(i, row[i])\n",
    "\n",
    "#         correct_tags.append(sec.split('\\n'))\n",
    "#     return correct_tags\n",
    "\n",
    "# tags = [i for i in list(data_p_df.columns.values) if i !='Reformulated Question' \\\n",
    "#                           and i !='Pattern' and i != 'Class'and i != 'train-test']\n",
    "\n",
    "\n",
    "# b = correct_entities(test, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab6b33",
   "metadata": {},
   "source": [
    "## Labeling task: few-shot learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ff9a2",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8837ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "dir_data = '/Users/mkazemi/cloudstor/2023/Visiting Research at UU/Simon Group/Paper/Dataset/'\n",
    "fr_dataset = pd.read_csv(dir_data + \"train_fr.csv\")\n",
    "fr_dataset = fr_dataset.fillna('')\n",
    "# fr_dataset.head()\n",
    "\n",
    "# fr_dataset.iloc[0]['Reformulated Question']\n",
    "\n",
    "\n",
    "prompt_completion = []\n",
    "for index, i in fr_dataset.iterrows():\n",
    "    \n",
    "    a = {\"prompt\":  i['Reformulated Question'] + \"\\n\\n###\\n\\n\",\\\n",
    "            \"completion\":\"Measure:\" + str([i['Measure']])  +\\\n",
    "            \"\\nCondition:\" + str([i['Condition']]) +\\\n",
    "                \"\\nSupport:\" + str([i['Support']]) +\\\n",
    "                    \"\\nSpatial_Extent:\" + str([i['Spatial Extent']]) +\\\n",
    "                        \"\\nTemporal_Extent:\" + str([i['Temporal extent']]) +\\\n",
    "                            \"\\nTime_measure:\" + str([i['Time_measure']])+\\\n",
    "                             \"\\nPlace_measure:\" + str([i['Place_measure']])+\\\n",
    "                                \"\\nSpatialRelation_Condition:\" + str([i['Spatial Relation_Condition']])+\\\n",
    "                                 \"\\nTemporalRelation_Condition:\" + str([i['Temporal Relation_Condition']])+\\\n",
    "                                \"\\nSpatialRelation_Support:\" + str([i['Spatial Relation_Support']])+\\\n",
    "                                \"\\nTemporalRelation_Support:\" + str([i['Temporal Relation_Support']])+\\\n",
    "                                \"\\nSpatialRelation_SpatialExtent:\" + str([i['Spatial Relation_Spatial Extent']])+\\\n",
    "                                \"\\nTemporalRelation_TemporalExtent:\" + str([i['Temporal Relation_Temporal Extent']]) + \"\\n END\"}\n",
    "    \n",
    "    prompt_completion.append(a)\n",
    "\n",
    "trainFR_gpt = [i for index,i in enumerate(prompt_completion) if index in [0,3,4,11,17,20,23,28,31,35]]\n",
    "testFR_gpt = [i for i in prompt_completion if i not in trainFR_gpt]\n",
    "\n",
    "file_name = \"/Users/mkazemi/cloudstor/2023/Visiting Research at UU/Simon Group/Paper/Dataset/trainFR_gpt.jsonl\"\n",
    "\n",
    "with open(dir_data + 'trainFR_gpt.jsonl', \"w\") as output_file:\n",
    "  for entry in trainFR_gpt:\n",
    "    json.dump(entry, output_file)\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "with open(dir_data + 'testFR_gpt.jsonl', \"w\") as output_file:\n",
    "  for entry in testFR_gpt:\n",
    "    json.dump(entry, output_file)\n",
    "    output_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### trained on 20 fr queries:  curie:ft-home-2023-09-29-06-02-30 \n",
    "## trained on 10 fr queries:  curie:ft-home-2023-09-29-14-49-11\n",
    "\n",
    "\n",
    "\n",
    "completion_fr = []\n",
    "for i in testFR_gpt:\n",
    "    \n",
    "    response = openai.Completion().create(\n",
    "    \n",
    "    model = 'curie:ft-home-2023-09-29-14-49-11',\n",
    "    prompt = i['prompt'],\n",
    "    max_tokens = 200,\n",
    "    temperature = 0,\n",
    "    # top_p =1,\n",
    "    # frequency_penalty = 0,\n",
    "    # presence_penalty = 0,\n",
    "    stop = ['END']\n",
    "    )\n",
    "    \n",
    "    completion_fr.append(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee358d7",
   "metadata": {},
   "source": [
    "### Evaluation: Fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_m_positive : the number of cases where GPT predicts a role which is the same with its true_m role.\n",
    "# false_positive: the number of cases where GPT predicts is not the same with true_m role (true_m role existed, \n",
    "# but model indetifies a null or different part of senetence )\n",
    "# false_negative: the number of cases where GPT predicts is not the same with true_m role (true_m role not existed, \n",
    "# but model indetifies a different part of senetence )\n",
    "# true_m_negative: the number of cases where GPT predicted_m nothing and there is a no true_m role in benchmark as well.\n",
    "\n",
    "\n",
    "\n",
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "\n",
    "\n",
    "    \n",
    "entities = {'Measure':0, 'Condition':1, 'Support':2, 'Spatial_Extent':3, 'Temporal_Extent':4,\\\n",
    "               'Time_measure':5, 'Place_measure':6, 'SpatialRelation_Condition':7,'TemporalRelation_Condition':8,\\\n",
    "               'SpatialRelation_Support':9,'TemporalRelation_Support':10, 'SpatialRelation_SpatialExtent':11,\\\n",
    "               'TemporalRelation_TemporalExtent':12}\n",
    "\n",
    "\n",
    "\n",
    "for index, m in enumerate(completion_fr):\n",
    "    \n",
    "        predicted = eval(m.split(\"\\n\")[0].split(\":\")[1])[0].rstrip()\n",
    "        true= eval(testFR_gpt[index]['completion'].split(\"\\n\")[0].split(\":\")[1])[0]\n",
    "    \n",
    "        if predicted in true and true != '' and predicted != '': \n",
    "            tp.append((predicted, true))\n",
    "        elif predicted not in true and true !='' and predicted != '' or true !='' and predicted == '':\n",
    "            fp.append((predicted, true))\n",
    "        \n",
    "        elif predicted not in true and true =='' and predicted != '':\n",
    "            fn.append((predicted, true))\n",
    "    \n",
    "        else:\n",
    "            tn.append((predicted, true))\n",
    "   \n",
    "accuracy = (len(tp) + len(tn))/ (len(tp) + len(fp) + len(fn) + len(tn))\n",
    "precision = (len(tp))/(len(tp) + len(fp))\n",
    "recall = (len(tp))/(len(tp) + len(fn))\n",
    "f1 = (2* recall_se* precision_se)/(precision_se + recall_se)\n",
    "    \n",
    "print(\"The evaluation metrics for are as below:\\n\\\n",
    "            accuracy: {0}\\n\\\n",
    "            precision: {1}\\n\\\n",
    "            recall: {2}\\n\\\n",
    "            f1: {3}\".format(accuracy,precision,recall,f1))\n",
    "\n",
    "# print(accuracy)\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "# print(f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd8691",
   "metadata": {},
   "source": [
    "## Labeling task: zero-shot learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_completion[0]['prompt']\n",
    "    \n",
    "# prompt = []\n",
    "# data_p = [data.loc[i] for i in range(len(data)) if data.loc[i]['Pattern'] == pattern]\n",
    "# data_p_df = pd.DataFrame(data_p)\n",
    "\n",
    "test = []\n",
    "for index, row in enumerate(prompt_completion):\n",
    "    \n",
    "    prompt = \"Extract 'Measure', 'Support', 'Condition', 'Spatial extent', and 'Temporal extent' from\\\n",
    "    the below question:\\n\\\n",
    "    '{0}'\\n\\\n",
    "    If there is a spatial relation in the extracted 'Support', identify that and put it under 'Support_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Support', identify that and put it under 'Support_TR' label.\\\n",
    "    \\nIf there is a spatial relation in the extracted 'Condition', identify that and put it under 'Condition_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Condition', identify that and put it under 'Condition_TR' label.\\\n",
    "    \\nIf there is a spatial relation in the extracted 'Spatial extent', identify that and put it under 'SE_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Temporal extent', identify that and put it under 'TE_TR' label.\\\n",
    "    \\nIn the cases where there are nothing found for each label, put ''.\\\n",
    "    \\nSo, order the outputs as below in JSON format:\\n\\\n",
    "    - Measure: measure\\n\\\n",
    "    - Support: support\\n\\\n",
    "    - Condition: condition\\n\\\n",
    "    - Spatial extent: spatial_extent\\n\\\n",
    "    -Temporal extent: temporal_extent\\n\\\n",
    "    - Support_SR: support_sr\\n\\\n",
    "    - Support_TR: support_tr\\n\\\n",
    "    - Condition_SR: condition_sr\\n\\\n",
    "    - Conidition_TR: condition_tr\\n\\\n",
    "    - SE_SR: se_sr\\n\\\n",
    "    - TE_TR: te_tr\\n\\\n",
    "    \".format(row['prompt'])\n",
    "    \n",
    "    \n",
    "    test.append(get_completion(prompt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = \"What is the crime rate for each suburb in Melbourne in 2020?\"\n",
    "# prompt = \"Extract 'Measure', 'Support', 'Condition', 'Spatial extent', and 'Temporal extent' from\\\n",
    "# the below question:\\n\\\n",
    "# '{0}'\\n\\\n",
    "# If there is a spatial relation in the extracted 'Support', identify that and put it under 'Support_SR' label.\\\n",
    "# \\nIf there is a temporal relation in the extracted 'Support', identify that and put it under 'Support_TR' label.\\\n",
    "# \\nIf there is a spatial relation in the extracted 'Condition', identify that and put it under 'Condition_SR' label.\\\n",
    "# \\nIf there is a temporal relation in the extracted 'Condition', identify that and put it under 'Condition_TR' label.\\\n",
    "# \\nIf there is a spatial relation in the extracted 'Spatial extent', identify that and put it under 'SE_SR' label.\\\n",
    "# \\nIf there is a temporal relation in the extracted 'Temporal extent', identify that and put it under 'TE_TR' label.\\\n",
    "# \\nIn the cases where there are nothing found for each label, put ''.\\\n",
    "# \\nSo, order the outputs as below in JSON format:\\n\\\n",
    "# - Measure: measure\\n\\\n",
    "# - Support: support\\n\\\n",
    "# - Condition: condition\\n\\\n",
    "# - Spatial extent: spatial_extent\\n\\\n",
    "# -Temporal extent: temporal_extent\\n\\\n",
    "# - Support_SR: support_sr\\n\\\n",
    "# - Support_TR: support_tr\\n\\\n",
    "# - Condition_SR: condition_sr\\n\\\n",
    "# - Conidition_TR: condition_tr\\n\\\n",
    "# - SE_SR: se_sr\\n\\\n",
    "# - TE_TR: te_tr\".format(p)\n",
    "    \n",
    "    \n",
    "# print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3c8a7",
   "metadata": {},
   "source": [
    "### Evaluation - Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "\n",
    "\n",
    "    \n",
    "prompt_completion = {'Measure':0, 'Condition':1, 'Support':2, 'Spatial_Extent':3, 'Temporal_Extent':4,\\\n",
    "               'Time_measure':5, 'Place_measure':6, 'SpatialRelation_Condition':7,'TemporalRelation_Condition':8,\\\n",
    "               'SpatialRelation_Support':9,'TemporalRelation_Support':10, 'SpatialRelation_SpatialExtent':11,\\\n",
    "               'TemporalRelation_TemporalExtent':12}\n",
    "\n",
    "\n",
    "\n",
    "for index, m in enumerate(test):\n",
    "    \n",
    "        predicted = eval(m)['TE_TR']\n",
    "        true= eval(prompt_completion[index]['completion'].split(\"\\n\")[-2].split(\":\")[1])[0]\n",
    "    \n",
    "        if predicted in true and true != '' and predicted != '': \n",
    "            tp.append((predicted, true))\n",
    "        elif predicted not in true and true !='' and predicted != '' or true !='' and predicted == '':\n",
    "            fp.append((predicted, true))\n",
    "        \n",
    "        elif predicted not in true and true =='' and predicted != '':\n",
    "            fn.append((predicted, true))\n",
    "    \n",
    "        else:\n",
    "            tn.append((predicted, true))\n",
    "   \n",
    "# accuracy = (len(tp) + len(tn))/ (len(tp) + len(fp) + len(fn) + len(tn))\n",
    "# precision = (len(tp))/(len(tp) + len(fp))\n",
    "# recall = (len(tp))/(len(tp) + len(fn))\n",
    "# f1 = (2* recall_se* precision_se)/(precision_se + recall_se)\n",
    "\n",
    "# print()\n",
    "# print(\"The evaluation metrics for are as below:\\n\\\n",
    "#             accuracy: {0}\\n\\\n",
    "#             precision: {1}\\n\\\n",
    "#             recall: {2}\\n\\\n",
    "#             f1: {3}\".format(accuracy,precision,recall,f1))\n",
    "\n",
    "print(\"{0},{1},{2},{3}\".format(len(tp),len(fp),len(fn),len(tn)))\n",
    "# print(accuracy)\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "# print(f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
