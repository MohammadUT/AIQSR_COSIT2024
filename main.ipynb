{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9e0d30",
   "metadata": {},
   "source": [
    "## Importing OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff19da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "api_key =\"API key in here\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a4269",
   "metadata": {},
   "source": [
    "## Labeling task: instruction-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2bb579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "data = pd.read_csv(\"Dataset/SignleFR_questions3.csv\")\n",
    "\n",
    "data_p_df = pd.DataFrame(data)\n",
    "data_p_df = data_p_df.fillna('\"\"')\n",
    "\n",
    "### first section of the prompot\n",
    "       \n",
    "sec1 = f\"\"\"If the functional roles as lists of \"Measure (M)\",\\\n",
    "\"Condition (C)\", \"Support (S)\", \"Spatial extent (SE)\", and\\\n",
    "\"Temporal extent (TE)\". Also, Spatial relation (SR) can present\\\n",
    "in the functional roles of C, S, SE, they are denoted as SRc, SRs,\\\n",
    "SRse, respectively. Also, Temporal Relation (TR) can present in the\\\n",
    "functional roles of C, S, TE, they are denoted as TRc, TRs, TRte,\\\n",
    "respectively. Also, Time (T) or Place (P) can only be present in the\\\n",
    "functional role of M, they are denoted as Tm and Pm, respectively.\\\n",
    "Also, SR in here is based on 'Cardinal Direction Calculus (CDC)' or\\\n",
    "'Region Connection Calculus (RCC)'. Also, TR is based on 'Allen Interval Algebra (AIA)'.\"\"\"\n",
    "    \n",
    "sec5 = ''\n",
    "sec4 = ''\n",
    "sec6 = ''\n",
    "sec7 = ''\n",
    "### second section of the prompt: Choosing one sample query has both SE and TE \n",
    "pattern = data_p_df.iloc[31]   \n",
    "pattern = pattern.fillna('\"\"')\n",
    "\n",
    "main_query = pattern['Reformulated Question']\n",
    "\n",
    "\n",
    "sec2 = \"In the question, '{0}', the functional roles are identified as below:\\\n",
    "    \\n- M: {1}\\n- C: {2}\\n- S: {3}\\n- SE: {4}\\n- TE {5}\\\n",
    "    \\nAlso, the spatial and temporal relations in these roles are identified in the given question as below:\\\n",
    "    \\n- Tm: {6}\\n- Pm: {7}\\n- SRc: {8}\\n- TRc: {9}\\n- SRs: {10}\\n- TRs: {11}\\n- SRse: {12}\\n- TRte: {13}\".format(\n",
    "        main_query, pattern['M'],\\\n",
    "        pattern['C'], pattern['S'],\\\n",
    "        pattern['SE'], pattern['TE'],\\\n",
    "        pattern['Tm'], pattern['Pm'],\\\n",
    "        pattern['SRc'], pattern['TRc'],\\\n",
    "        pattern['SRs'], pattern['TRs'],\\\n",
    "        pattern['SRse'], pattern['TRte'])\n",
    "\n",
    "  \n",
    "    \n",
    "### Third section of the prompt: instruct based on training set\n",
    "prompt = []\n",
    "training = [data.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'train']\n",
    "training_df = pd.DataFrame(training)\n",
    "\n",
    "for index, row in training_df.iterrows():\n",
    "    \n",
    "    Notnan_columns = [i for i in row[~row.isna()].index.tolist() if i !='Reformulated Question' \\\n",
    "                          and i !='Pattern' and i != 'Class'and i != 'train-test']\n",
    "    sec3 = ''\n",
    "\n",
    "    for i in Notnan_columns:\n",
    "        sec3 += \"{0}: {1}\\n\".format(i, row[i])\n",
    "\n",
    "    sec4 += \"In the question, '{0}', the functional roles are as below:\\n{1}\\n\".format(row['Reformulated Question'],sec3)\n",
    "\n",
    "sec5 = \"{0}\\n\\n{1}\\n\\n{2}\".format(sec1, sec2, sec4)\n",
    "    \n",
    "### fourth section of the prompt: ask to label based on test set\n",
    "\n",
    "test = [data.loc[i] for i in range(len(data_p_df)) if data_p_df.loc[i]['train-test'] == 'test']\n",
    "test_df = pd.DataFrame(test)\n",
    "response_test = []\n",
    "for index, row in test_df.iterrows():\n",
    " \n",
    "    sec6 = \"In the question, '{0}', the functional roles are as below:\\n\".format(row['Reformulated Question'])\n",
    "    sec7 = sec5 + sec6 \n",
    "    response = get_completion(sec7)\n",
    "    response_test.append(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4f154-b088-4047-a34e-18204a6a0f75",
   "metadata": {},
   "source": [
    "## Labeling task: Zero-shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36ef7d-42c7-44a5-b5b1-5bd9a98318f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_completion[0]['prompt']\n",
    "    \n",
    "# prompt = []\n",
    "# data_p = [data.loc[i] for i in range(len(data)) if data.loc[i]['Pattern'] == pattern]\n",
    "# data_p_df = pd.DataFrame(data_p)\n",
    "\n",
    "test = []\n",
    "for index, row in enumerate(prompt_completion):\n",
    "    \n",
    "    prompt = \"Extract 'Measure', 'Support', 'Condition', 'Spatial extent', and 'Temporal extent' from\\\n",
    "    the below question:\\n\\\n",
    "    '{0}'\\n\\\n",
    "    If there is a spatial relation in the extracted 'Support', identify that and put it under 'Support_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Support', identify that and put it under 'Support_TR' label.\\\n",
    "    \\nIf there is a spatial relation in the extracted 'Condition', identify that and put it under 'Condition_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Condition', identify that and put it under 'Condition_TR' label.\\\n",
    "    \\nIf there is a spatial relation in the extracted 'Spatial extent', identify that and put it under 'SE_SR' label.\\\n",
    "    \\nIf there is a temporal relation in the extracted 'Temporal extent', identify that and put it under 'TE_TR' label.\\\n",
    "    \\nIn the cases where there are nothing found for each label, put ''.\\\n",
    "    \\nSo, order the outputs as below in JSON format:\\n\\\n",
    "    - Measure: measure\\n\\\n",
    "    - Support: support\\n\\\n",
    "    - Condition: condition\\n\\\n",
    "    - Spatial extent: spatial_extent\\n\\\n",
    "    -Temporal extent: temporal_extent\\n\\\n",
    "    - Support_SR: support_sr\\n\\\n",
    "    - Support_TR: support_tr\\n\\\n",
    "    - Condition_SR: condition_sr\\n\\\n",
    "    - Conidition_TR: condition_tr\\n\\\n",
    "    - SE_SR: se_sr\\n\\\n",
    "    - TE_TR: te_tr\\n\\\n",
    "    \".format(row['prompt'])\n",
    "    \n",
    "    \n",
    "    test.append(get_completion(prompt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e37331-2dde-4512-b348-6e1a4aaeba40",
   "metadata": {},
   "source": [
    "## Labeling task: Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb2a98-4db5-4c15-a2e1-ac3876914dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "dir_data = '/Users/mkazemi/cloudstor/2023/Visiting Research at UU/Simon Group/Paper/Dataset/'\n",
    "fr_dataset = pd.read_csv(dir_data + \"train_fr.csv\")\n",
    "fr_dataset = fr_dataset.fillna('')\n",
    "# fr_dataset.head()\n",
    "\n",
    "# fr_dataset.iloc[0]['Reformulated Question']\n",
    "\n",
    "\n",
    "prompt_completion = []\n",
    "for index, i in fr_dataset.iterrows():\n",
    "    \n",
    "    a = {\"prompt\":  i['Reformulated Question'] + \"\\n\\n###\\n\\n\",\\\n",
    "            \"completion\":\"Measure:\" + str([i['Measure']])  +\\\n",
    "            \"\\nCondition:\" + str([i['Condition']]) +\\\n",
    "                \"\\nSupport:\" + str([i['Support']]) +\\\n",
    "                    \"\\nSpatial_Extent:\" + str([i['Spatial Extent']]) +\\\n",
    "                        \"\\nTemporal_Extent:\" + str([i['Temporal extent']]) +\\\n",
    "                            \"\\nTime_measure:\" + str([i['Time_measure']])+\\\n",
    "                             \"\\nPlace_measure:\" + str([i['Place_measure']])+\\\n",
    "                                \"\\nSpatialRelation_Condition:\" + str([i['Spatial Relation_Condition']])+\\\n",
    "                                 \"\\nTemporalRelation_Condition:\" + str([i['Temporal Relation_Condition']])+\\\n",
    "                                \"\\nSpatialRelation_Support:\" + str([i['Spatial Relation_Support']])+\\\n",
    "                                \"\\nTemporalRelation_Support:\" + str([i['Temporal Relation_Support']])+\\\n",
    "                                \"\\nSpatialRelation_SpatialExtent:\" + str([i['Spatial Relation_Spatial Extent']])+\\\n",
    "                                \"\\nTemporalRelation_TemporalExtent:\" + str([i['Temporal Relation_Temporal Extent']]) + \"\\n END\"}\n",
    "    \n",
    "    prompt_completion.append(a)\n",
    "\n",
    "trainFR_gpt = [i for index,i in enumerate(prompt_completion) if index in [0,3,4,11,17,20,23,28,31,35]]\n",
    "testFR_gpt = [i for i in prompt_completion if i not in trainFR_gpt]\n",
    "\n",
    "file_name = \"/Users/mkazemi/cloudstor/2023/Visiting Research at UU/Simon Group/Paper/Dataset/trainFR_gpt.jsonl\"\n",
    "\n",
    "with open(dir_data + 'trainFR_gpt.jsonl', \"w\") as output_file:\n",
    "  for entry in trainFR_gpt:\n",
    "    json.dump(entry, output_file)\n",
    "    output_file.write(\"\\n\")\n",
    "    \n",
    "with open(dir_data + 'testFR_gpt.jsonl', \"w\") as output_file:\n",
    "  for entry in testFR_gpt:\n",
    "    json.dump(entry, output_file)\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "completion_fr = []\n",
    "for i in testFR_gpt:\n",
    "    \n",
    "    response = openai.Completion().create(\n",
    "    \n",
    "    model = 'curie:ft-home-2023-09-29-14-49-11',\n",
    "    prompt = i['prompt'],\n",
    "    max_tokens = 200,\n",
    "    temperature = 0,\n",
    "    # top_p =1,\n",
    "    # frequency_penalty = 0,\n",
    "    # presence_penalty = 0,\n",
    "    stop = ['END']\n",
    "    )\n",
    "    \n",
    "    completion_fr.append(response['choices'][0]['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee8348-fbb3-490d-bb6e-8793c148db7e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8120d9af-2783-4214-8949-f41b46f4ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_m_positive : the number of cases where GPT predicts a role which is the same with its true_m role.\n",
    "# false_positive: the number of cases where GPT predicts is not the same with true_m role (true_m role existed, \n",
    "# but model indetifies a null or different part of senetence )\n",
    "# false_negative: the number of cases where GPT predicts is not the same with true_m role (true_m role not existed, \n",
    "# but model indetifies a different part of senetence )\n",
    "# true_m_negative: the number of cases where GPT predicted_m nothing and there is a no true_m role in benchmark as well.\n",
    "\n",
    "\n",
    "\n",
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "\n",
    "\n",
    "    \n",
    "entities = {'Measure':0, 'Condition':1, 'Support':2, 'Spatial_Extent':3, 'Temporal_Extent':4,\\\n",
    "               'Time_measure':5, 'Place_measure':6, 'SpatialRelation_Condition':7,'TemporalRelation_Condition':8,\\\n",
    "               'SpatialRelation_Support':9,'TemporalRelation_Support':10, 'SpatialRelation_SpatialExtent':11,\\\n",
    "               'TemporalRelation_TemporalExtent':12}\n",
    "\n",
    "\n",
    "\n",
    "for index, m in enumerate(completion_fr):\n",
    "    \n",
    "        predicted = eval(m.split(\"\\n\")[0].split(\":\")[1])[0].rstrip()\n",
    "        true= eval(testFR_gpt[index]['completion'].split(\"\\n\")[0].split(\":\")[1])[0]\n",
    "    \n",
    "        if predicted in true and true != '' and predicted != '': \n",
    "            tp.append((predicted, true))\n",
    "        elif predicted not in true and true !='' and predicted != '' or true !='' and predicted == '':\n",
    "            fp.append((predicted, true))\n",
    "        \n",
    "        elif predicted not in true and true =='' and predicted != '':\n",
    "            fn.append((predicted, true))\n",
    "    \n",
    "        else:\n",
    "            tn.append((predicted, true))\n",
    "   \n",
    "accuracy = (len(tp) + len(tn))/ (len(tp) + len(fp) + len(fn) + len(tn))\n",
    "precision = (len(tp))/(len(tp) + len(fp))\n",
    "recall = (len(tp))/(len(tp) + len(fn))\n",
    "f1 = (2* recall_se* precision_se)/(precision_se + recall_se)\n",
    "    \n",
    "print(\"The evaluation metrics for are as below:\\n\\\n",
    "            accuracy: {0}\\n\\\n",
    "            precision: {1}\\n\\\n",
    "            recall: {2}\\n\\\n",
    "            f1: {3}\".format(accuracy,precision,recall,f1))\n",
    "\n",
    "# print(accuracy)\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "# print(f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5a9d4-80c4-4f25-ac8a-d5bfc1e70807",
   "metadata": {},
   "source": [
    "## Generated Questions With Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f3471b-36f0-46c3-92f5-95a6729d519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "data = pd.read_csv(\"Dataset/SignleFR_questions2.csv\")\n",
    "\n",
    "\n",
    "def instructed_prompt(pattern, number_of_queries):\n",
    "    \n",
    "    prompt = []\n",
    "    data_p = [data.loc[i] for i in range(len(data)) if data.loc[i]['Pattern'] == pattern]\n",
    "    data_p_df = pd.DataFrame(data_p)\n",
    "\n",
    "    \n",
    "    ### first section of the prompot\n",
    "       \n",
    "    sec1 = f\"\"\"If the functional roles as lists of \"Measure (M)\", \"Condition (C)\", \"Support (S)\", \"Spatial extent (SE)\", and \"Temporal extent (TE)\". Also, Spatial relation (SR) can present in the functional roles of C, S, SE, they are denoted as SRc, SRs, SRse, respectively. Also, Temporal Relation (TR) can present in the functional roles of C, S, TE, they are denoted as TRc, TRs, TRte, respectively. Also, Time (T) or Place (P) can only be present in the functional role of M, they are denoted as Tm and Pm, respectively. Also, SR in here is based on 'Cardinal Direction Calculus (CDC)' or 'Region Connection Calculus (RCC)'. Also, TR is based on 'Allen Interval Algebra (AIA)'.\"\"\"\n",
    "    \n",
    "\n",
    "    ### second section of the prompt: Choosing one sample query has both SE and TE \n",
    "        \n",
    "        \n",
    "    first_shot_class = [data_p_df.iloc[i] for i in range(len(data_p_df)) if data_p_df.iloc[i]['Class'] == pattern]\n",
    "    first_shot_class_df = pd.DataFrame(first_shot_class[0])\n",
    "    \n",
    "    first_shot_class_df_na = first_shot_class_df.fillna('\"\"')\n",
    "    \n",
    "    sec2 = \"In the question, '{0}', the functional roles are identified as below:\\\n",
    "    \\n- M: {1}\\n- C: {2}\\n- S: {3}\\n- SE: {4}\\n- TE {5}\\\n",
    "    \\nAlso, the spatial and temporal relations in these roles are identified in the given question as below:\\\n",
    "    \\n- Tm: {6}\\n- Pm: {7}\\n- SRc: {8}\\n- TRc: {9}\\n- SRs: {10}\\n- TRs: {11}\\n- SRse: {12}\\n- TRte: {13}\".format(\n",
    "        list(first_shot_class_df_na.loc['Reformulated Question'])[0], list(first_shot_class_df_na.loc['M'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['C'])[0], list(first_shot_class_df_na.loc['S'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['SE'])[0], list(first_shot_class_df_na.loc['TE'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['Tm'])[0], list(first_shot_class_df_na.loc['Pm'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['SRc'])[0], list(first_shot_class_df_na.loc['TRc'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['SRs'])[0], list(first_shot_class_df_na.loc['TRs'])[0],\\\n",
    "        list(first_shot_class_df_na.loc['SRse'])[0], list(first_shot_class_df_na.loc['TRte'])[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Third section of the prompt: Few shot learning\n",
    "    prompt = []\n",
    "    \n",
    "    for index, row in data_p_df.iterrows():\n",
    "    \n",
    "    \n",
    "        Notnan_columns = [i for i in row[~row.isna()].index.tolist() if i !='Reformulated Question' \\\n",
    "                          and i !='Pattern' and i != 'Class']\n",
    "        \n",
    "        Notnan_columns_format = \"'{0}'\".format(', '.join(map(str, Notnan_columns)).replace(', ', \"','\"))\n",
    "        \n",
    "        sec3 = \"Generate a new 'What' question that has only {0}:\".format(Notnan_columns_format)\n",
    "    \n",
    "    \n",
    "        if row['Class'] != pattern:\n",
    "        \n",
    "            sec5 =''\n",
    "            for i in Notnan_columns:\n",
    "            \n",
    "                sec5 += \"- {0}: {1}\\n\".format(i, row[i])\n",
    "        \n",
    "                sec4 = \"{0}\\nQuestion: {1}\\n{2}\\nGenerate {3} new 'What' questions that has only {4}:\".format(\n",
    "                    sec3, row['Reformulated Question'], sec5, number_of_queries, Notnan_columns_format)\n",
    "            \n",
    "\n",
    "            prompt.append(\"{0}\\n\\n{1}\\n\\n{2}\".format(sec1, sec2, sec4))\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt = instructed_prompt('M_C_S_SE_TE_SRse_TRte',2)[11]\n",
    "\n",
    "response = get_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40680499-c8cd-4127-b045-3da7d17d69f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
